{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_articles_path = '/input/dev-articles/dev-articles/'\n",
    "test_articles_path = '/input/testarticles/test-articles/'\n",
    "data_path = '/input/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel,BertModel, BertConfig\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "class BertForTokenClassificationWeighted(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.cnn1 = nn.Conv1d(768, 128, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(128, config.num_labels, kernel_size=3, padding=1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output=sequence_output.permute(0,2,1)\n",
    "#         sequence_output = self.dropout(sequence_output)\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.cnn2(self.cnn1((dropout(sequence_output))))\n",
    "            else:\n",
    "                logits += self.cnn2(self.cnn1(dropout(sequence_output)))\n",
    "        logits=logits/ len(self.dropouts)\n",
    "        logits=logits.permute(0,2,1)\n",
    "\n",
    "#         logits = self.classifier(sequence_output)\n",
    "#         print(logits.shape)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            w = torch.tensor([1.,10.,1. ]).cuda()\n",
    "            loss_fct = CrossEntropyLoss(weight=w)\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.reshape(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south\n",
      "florida\n",
      "muslim\n",
      "leader\n",
      "sofian\n",
      "zakkout\n",
      "david\n",
      "duke\n",
      "day\n",
      "south florida muslim leader sofian zakkout david duke day\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99a177712a847db888d9d3aa7b26999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "south florida muslim leader sofian zakkout david duke day\n",
      "['south', 'florida', 'muslim', 'leader', 'sofia', '##n', 'za', '##kko', '##ut', 'david', 'duke', 'day']\n",
      "(14885, 128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00c8ca416ba41c78612e3e3f8635746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb785c599564d258724d71dc245002a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5036991468506822\n",
      "Validation loss: 0.6081043768387574\n",
      "Validation Accuracy: 0.6469854608282343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  14%|█▍        | 1/7 [03:22<20:13, 202.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4412132930614571\n",
      "Train loss: 0.3058424625114054\n",
      "Validation loss: 0.7097693873712649\n",
      "Validation Accuracy: 0.7329933860085227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  29%|██▊       | 2/7 [06:44<16:51, 202.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4578833283953121\n",
      "Train loss: 0.2055117714352698\n",
      "Validation loss: 0.958174603489729\n",
      "Validation Accuracy: 0.7646859975961539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  43%|████▎     | 3/7 [10:06<13:28, 202.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.46052171257271723\n",
      "Train loss: 0.138776995457485\n",
      "Validation loss: 1.136403695322\n",
      "Validation Accuracy: 0.7766100476671766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 4/7 [13:28<10:06, 202.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4866751515363561\n",
      "Train loss: 0.09758572739273182\n",
      "Validation loss: 1.7093272203436265\n",
      "Validation Accuracy: 0.8269153941761365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  71%|███████▏  | 5/7 [16:50<06:44, 202.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.49178370314466774\n",
      "Train loss: 0.07719366211651989\n",
      "Validation loss: 2.059458752664236\n",
      "Validation Accuracy: 0.8518984067690122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  86%|████████▌ | 6/7 [20:12<03:22, 202.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.49916521697148014\n",
      "Train loss: 0.060983501229826924\n",
      "Validation loss: 1.7349001346872404\n",
      "Validation Accuracy: 0.8194763610413024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 7/7 [23:33<00:00, 201.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.5273995019771422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig,WordpieceTokenizer\n",
    "from transformers import DistilBertForTokenClassification,DistilBertTokenizer,DistilBertConfig\n",
    "from transformers import RobertaForTokenClassification,RobertaTokenizer,RobertaConfig\n",
    "from transformers import BertForTokenClassification,BertTokenizer,BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "global max_len\n",
    "data = pd.read_csv(data_path + \"data_prop.csv\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "data['label'] = data['label'].astype(str)\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p) for w, p in zip(s[\"word_corrected\"].values.tolist(),\n",
    "                                                           s[\"label\"].values.tolist())]\n",
    "        \n",
    "        self.grouped = self.data.groupby(\"sent_id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "\n",
    "\n",
    "sent = getter.sentences\n",
    "\n",
    "for s in sent[0]:\n",
    "    print(s[0].split()[0])\n",
    "    \n",
    "\n",
    "##keeping only the first word after removing the special characters\n",
    "sentences = [\" \".join([s[0].split()[0] for s in sent]) for sent in getter.sentences]\n",
    "print(sentences[0])\n",
    "\n",
    "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])\n",
    "\n",
    "# tags_vals = list(set(data[\"label\"].values))\n",
    "# tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,WeightedRandomSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_LEN = 128\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print(n_gpu)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "### Now we tokenize all sentences\n",
    "\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "print(sentences[0])\n",
    "print(tokenized_texts[0])\n",
    "\n",
    "def reg_encoding(cleaned: list, labels: list, hash_token:list) -> list:\n",
    "    label_l = []\n",
    "    for oindex, x in enumerate(cleaned):\n",
    "        #print(oindex)\n",
    "        tlist = []        \n",
    "        i=0\n",
    "        j=0\n",
    "        while i < len(x): \n",
    "            if x[i][0]=='#':        \n",
    "                tlist.append(hash_token)\n",
    "            else:\n",
    "                #print(x[i])\n",
    "                tlist.append(labels[oindex][j])\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "            \n",
    "        label_l.append(tlist)\n",
    "    return label_l\n",
    "\n",
    "def reg_encoding_generic(sentences,labels, hash_token):\n",
    "    tokens_all=[]\n",
    "    labels_all=[]\n",
    "    for (sentence,label) in zip(sentences,labels):\n",
    "        tokens_per_sentence=[]\n",
    "        labels_per_sentence=[]\n",
    "        for word, label in zip(sentence.split(), label):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens_per_sentence.extend(word_tokens)\n",
    "                labels_per_sentence.extend([label] + [hash_token] * (len(word_tokens) - 1))\n",
    "#                 labels_per_sentence.extend([label] + [label] * (len(word_tokens) - 1))\n",
    "\n",
    "        \n",
    "        tokens_all.append(tokens_per_sentence)\n",
    "        labels_all.append(labels_per_sentence)\n",
    "                \n",
    "    return tokens_all,labels_all\n",
    "\n",
    "def reg_encoding_modify(cleaned: list, labels: list, hash_token, end_token) -> list:\n",
    "    label_l = []\n",
    "    for oindex, x in enumerate(cleaned):\n",
    "        #print(oindex)\n",
    "        tlist = []        \n",
    "        i=0\n",
    "        j=0\n",
    "        while i < len(x): \n",
    "            if x[i][0]=='#':        \n",
    "                #tlist.append(hash_token)\n",
    "                tlist.append(labels[oindex][j-1])\n",
    "                \n",
    "            else:\n",
    "                #print(x[i])\n",
    "                tlist.append(labels[oindex][j])\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "            \n",
    "        label_l.append(tlist)\n",
    "    return label_l\n",
    "\n",
    "\n",
    "tokenized_texts,label_l = reg_encoding_generic(sentences,labels,'X')\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in label_l for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#label_l_modify = reg_encoding_modify(tokenized_texts,labels,'X','E')\n",
    "\n",
    "# data['label'].unique()\n",
    "\n",
    "tags_vals=['0','1','X']\n",
    "tag2idx={'0':0,'1':1,'X':2}\n",
    "\n",
    "# tags_vals=['0','1']\n",
    "# tag2idx={'0':0,'1':1}\n",
    "\n",
    "\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in label_l],maxlen=MAX_LEN, value =0, padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2018, test_size=0.1)\n",
    "\n",
    "sent_labels=np.zeros(tr_tags.shape[0],dtype=int)\n",
    "for i in range(len(tr_tags)):\n",
    "    if 1 in tr_tags[i,:]:\n",
    "        sent_labels[i]=1\n",
    "    \n",
    "del input_ids,tags,attention_masks\n",
    "print(tr_inputs.shape)\n",
    "\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags) \n",
    "train_sampler = RandomSampler(train_data)\n",
    "_,class_sample_count=np.unique( sent_labels , return_counts=True)   \n",
    "class_sample_counts=list(class_sample_count)\n",
    "class_sample_count = [10, 1, 20, 3, 4] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
    "weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "samples_weights = weights[sent_labels]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, num_samples=len(samples_weights),replacement=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size = bs, sampler = sampler)\n",
    "\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "\n",
    "\n",
    "# model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(tag2idx))\n",
    "# model = RobertaForTokenClassification.from_pretrained(\"roberta-base\", num_labels=len(tag2idx))\n",
    "\n",
    "# model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "model = BertForTokenClassificationWeighted.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2, num_training_steps=5)\n",
    "# opt = SWA(optimizer, swa_start=10, swa_freq=2, swa_lr=0.05)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "epochs = 7\n",
    "max_grad_norm = 1.0\n",
    "tmp=0\n",
    "\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "#         loss = model(b_input_ids, token_type_ids=None,\n",
    "#                      attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         opt.step()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "#     opt.swap_swa_sgd()\n",
    "\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    pred=[]\n",
    "    \n",
    "    for batch in (valid_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "#             tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "#                                   attention_mask=b_input_mask, labels=b_labels)\n",
    "#             logits = model(b_input_ids, token_type_ids=None,\n",
    "#                            attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        #print(logits.shape)\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        #\n",
    "        pred.append(list(p) for p in np.argmax(logits, axis=2))\n",
    "        \n",
    "        #\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    \n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    \n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    \n",
    "    torch.save(model.state_dict(), 'bert_uncased_attentionmask.pth')\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    tmp_f1=f1_score(pred_tags , valid_tags,average='macro')\n",
    "    if tmp_f1>tmp:\n",
    "        torch.save(model.state_dict(), 'bert_uncased_SI.pth')\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags , valid_tags,average='macro')))\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = nn.Conv1d(768, 128, kernel_size=3, padding=1)\n",
    "cnn2 = nn.Conv1d(128, 3, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768, 128]) torch.Size([32, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(32, 128,768)\n",
    "inp=inp.permute(0,2,1)\n",
    "inp.size()\n",
    "output = cnn2(cnn1(inp))\n",
    "output=output.permute(0,2,1)\n",
    "print(inp.size()\n",
    ",output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 71/75 [02:30<00:07,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [02:41<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 3)\n",
      "(1330, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading the model\n",
    "# model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))    \n",
    "\n",
    "# model.load_state_dict(torch.load('/kaggle/output/bert-new/bert_uncased_SI.pth'))\n",
    "# model.load_state_dict(torch.load('/kaggle/working/bert-new/bert_uncased_SI.pth'))\n",
    "\n",
    "\n",
    "# model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# Fetching test data\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text= text.lower()\n",
    "    text= re.sub(r'[^a-z]',' ',text)\n",
    "    text= ' '.join(text.split())\n",
    "    return(text)\n",
    "def get_wordchar_indicies(sent):\n",
    "    k_l = list(sent)\n",
    "    k_l_b = [0 if i==' ' else 1 for i in k_l]\n",
    "    k_df = pd.DataFrame({'char':k_l, 'space_mark':k_l_b})\n",
    "    k_df = k_df.reset_index()\n",
    "    k_df['u1'] = k_df['space_mark'].diff()\n",
    "    k_df['u1'].fillna(1, inplace=True)\n",
    "    k_df.loc[k_df['u1']==1, 'u2']= k_df.loc[k_df['u1']==1, 'u1'].cumsum()\n",
    "    k_df.loc[k_df['u1']==-1, 'u2']= k_df.loc[k_df['u1']==-1, 'u1']\n",
    "    k_df['u2'] = k_df['u2'].ffill(axis=0)\n",
    "    k_df = k_df[k_df['u2']!=-1]\n",
    "    k_df_gb = pd.DataFrame(k_df.groupby(['u2'])['index'].min())\n",
    "    k_df_gb['last_index_word'] = k_df.groupby(['u2'])['index'].max()\n",
    "    k_df_gb = k_df_gb.reset_index().rename(columns={'u2':'word_index','index':'first_index_word'})\n",
    "    try:\n",
    "        k_df_gb['words'] = sent.split()\n",
    "    except:\n",
    "        print(k_df_gb, sent)\n",
    "    return k_df_gb\n",
    "\n",
    "def indices_sentence(article_id,path):\n",
    "    f= open(path + 'article' + str(article_id) + '.txt',\"r\")\n",
    "    indices={}\n",
    "    start_index = 0\n",
    "    for i, line in enumerate(f):\n",
    "        indices[i] = {}\n",
    "        indices[i]['article_id']=article_id\n",
    "        indices[i]['span_present'] = 0\n",
    "        indices[i]['sentence'] = line\n",
    "        indices[i]['start_index'] = start_index\n",
    "        indices[i]['end_index'] = start_index + len(line)\n",
    "        start_index = indices[i]['end_index']   \n",
    "        \n",
    "        if line == '\\n':\n",
    "            indices[i]['word_st_index'] = [0]\n",
    "            indices[i]['word_en_index'] = [0]\n",
    "        else:\n",
    "            wordchar_df = get_wordchar_indicies(line)\n",
    "            indices[i]['word_st_index'] = list(wordchar_df['first_index_word'])\n",
    "            indices[i]['word_en_index'] = list(wordchar_df['last_index_word'])        \n",
    "        \n",
    "    return indices\n",
    "def get_test_sentences(article_ids, dev_id, articles_path):\n",
    "    se_dict=indices_sentence(article_ids[dev_id],articles_path)\n",
    "    test_sent=[]\n",
    "    for i in range(len(se_dict)):\n",
    "                   test_sent.append(se_dict[i]['sentence'])\n",
    "    return se_dict, test_sent\n",
    "\n",
    "\n",
    "\n",
    "def prep_test_dataloader(article_ids, dev_id, articles_path):\n",
    "    se_dict,test_sentences=get_test_sentences(article_ids,dev_id,articles_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    tokenized_test_sentences = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
    "    input_test_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_sentences],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_test_masks = [[float(i>0) for i in ii] for ii in input_test_ids]\n",
    "    test_inputs = torch.tensor(input_test_ids)\n",
    "\n",
    "    test_masks = torch.tensor(attention_test_masks)\n",
    "    test_data = TensorDataset(test_inputs, test_masks)\n",
    "\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
    "    return test_dataloader,test_sentences\n",
    "\n",
    "\n",
    "def inverse_reg_enco(sentences,labels_tok):\n",
    "    label_inverse=[]\n",
    "    for i,sent in enumerate(sentences) :\n",
    "        label_o=[]\n",
    "        word_count=0\n",
    "        tok_count=0\n",
    "        for word in sent.split():            \n",
    "            #print(word)\n",
    "            tok=tokenizer.tokenize(word)\n",
    "            tok_count=len(tok)+tok_count\n",
    "            #print(tok_count)\n",
    "            if tok_count>MAX_LEN:\n",
    "                break    \n",
    "            #print(word_count)\n",
    "            if len(tok)>1:\n",
    "                #print('True')\n",
    "                label_word_token=labels_tok[i][word_count:word_count+len(tok)]\n",
    "                #print(label_word_token)\n",
    "                if '1' in label_word_token:\n",
    "                    label_o.append('1')\n",
    "                else:\n",
    "                    label_o.append('0')\n",
    "                word_count=word_count+len(tok)\n",
    "            else:\n",
    "                #print(labels_tok[i][word_count])\n",
    "                label_o.append(labels_tok[i][word_count])\n",
    "                word_count=word_count+1    \n",
    "        label_inverse.append(label_o)\n",
    "    return label_inverse\n",
    "\n",
    "# out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "# preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "# def inverse_reg_enco_modify(sentences,labels_tok):\n",
    "#     for i in range(labels_tok.shape[0]):\n",
    "#         for j in range(labels_tok.shape[1]):\n",
    "#             if labels_tok[i, j] != 2:\n",
    "#                 out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "#                 preds_list[i].append(label_map[preds[i][j]])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def thresh_logit(logits,thresh):\n",
    "    predictions_per_batch=[]\n",
    "    for i in range(logits.shape[0]):\n",
    "        predictions_per_sentence=[]\n",
    "        for j in range(logits.shape[1]):\n",
    "            if (np.argmax(logits[i][j])==1 and logits[i][j][1]>=thresh):\n",
    "                predictions_per_sentence.append(1)\n",
    "            else:\n",
    "                predictions_per_sentence.append(0)\n",
    "        predictions_per_batch.append(predictions_per_sentence)\n",
    "    return predictions_per_batch \n",
    "\n",
    "\n",
    "#from seqeval.metrics import f1_score\n",
    "def predict_bert(model,test_dataloader,test_sentences,thresh,threshold=False):\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    l=[]\n",
    "    #true_labels = []\n",
    "    #eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
    "        with torch.no_grad():\n",
    "            #tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "            #                      attention_mask=b_input_mask, labels=b_labels)\n",
    "#             logits = model(b_input_ids, token_type_ids=None,\n",
    "#                            attention_mask=b_input_mask)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "            \n",
    "\n",
    "        m=torch.nn.Softmax(dim=2)\n",
    "        logits_softmax=m(logits)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        logits_softmax = logits_softmax.detach().cpu().numpy()\n",
    "        #print(logits.shape)\n",
    "        #print(logits_softmax.shape)\n",
    "        #l.extend(logits_softmax)\n",
    "        if threshold:\n",
    "            predictions_per_batch=thresh_logit(logits,thresh)\n",
    "            predictions.extend(predictions_per_batch)\n",
    "        else:\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "            \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "    pred_tags_l=inverse_reg_enco(test_sentences,pred_tags)\n",
    "    padded_pred_tags_l=pad_sequences([[tag2idx.get(l) for l in lab] for lab in pred_tags_l],\n",
    "                     maxlen=MAX_LEN, value = tag2idx['0'], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "    return padded_pred_tags_l,pred_tags\n",
    "\n",
    "\n",
    "def get_spans_article(article_ids, dev_id, articles_path,padded_pred_tags_l):\n",
    "    pad_style='Post'\n",
    "    spangaps_to_merge = [1, 2]\n",
    "    def get_truewordindex(wi, sl):\n",
    "        if sl>=max_len:\n",
    "            return wi\n",
    "        else:\n",
    "            return wi-(max_len-sl)\n",
    "    ar=np.array(padded_pred_tags_l)\n",
    "    #print(padded_pred_tags_l.shape)\n",
    "    a,b=np.where(ar==1)\n",
    "    \n",
    "    df_pred=pd.DataFrame({'sent_id':a,'word_index':b})\n",
    "    #print(len(df_pred))\n",
    "    meta_dict, test_sentences= get_test_sentences(article_ids, dev_id, articles_path)\n",
    "    org_sent_len = len(test_sentences)\n",
    "\n",
    "    len_sentence={i:len(test_sentences[i].split()) for i in range(len(test_sentences))}\n",
    "    #print(len_sentence)\n",
    "    df_pred['sent_length']=df_pred['sent_id'].map(len_sentence)\n",
    "    df_pred = df_pred[df_pred['sent_id']<org_sent_len]\n",
    "\n",
    "    # if len(df_pred)==0:\n",
    "    #     return df_pred, df_pred, df_pred\n",
    "    # df_pred['true_word_index']=df_pred['word_index']-(max_len-df_pred['sent_length'])\n",
    "    if pad_style == 'pre':\n",
    "        df_pred['true_word_index'] = list(map(lambda x, y: get_truewordindex(x, y) , \n",
    "                                          df_pred['word_index'], df_pred['sent_length']))\n",
    "    else:\n",
    "        df_pred['true_word_index'] = df_pred['word_index'].values\n",
    "    #print(df_pred)\n",
    "    df_pred = df_pred[~(df_pred['true_word_index']>=df_pred['sent_length'])] \n",
    "    \n",
    "    #print(len(df_pred))\n",
    "    df_pred['diff_pred']=df_pred.groupby(['sent_id'])['true_word_index'].diff()\n",
    "    df_pred['diff_pred'] = df_pred['diff_pred'].apply(\n",
    "                        lambda x:-1 if x in spangaps_to_merge else np.nan)\n",
    "    #print (len(df_pred))\n",
    "    #print(df_pred)\n",
    "    if len(df_pred)<1:\n",
    "        print(\"Escaping empty dataframe :\")\n",
    "        print(dev_id)\n",
    "        submsn_df=pd.DataFrame()\n",
    "        sent_pred=pd.DataFrame()\n",
    "        return df_pred, sent_pred, submsn_df\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        df_pred.loc[df_pred['diff_pred'].isnull(), 'diff_pred_1'] = \\\n",
    "                    df_pred.groupby(['sent_id'])['diff_pred'].cumcount()\n",
    "        df_pred['diff_pred_1'] = df_pred['diff_pred_1'].ffill(axis=0)\n",
    "        df_pred['span_id'] = list(map(lambda x, y: str(int(x)) + '_' + str(int(y)), \n",
    "                                      df_pred['sent_id'], df_pred['diff_pred_1']))\n",
    "        # req_cols = ['sent_id', 'word_index', 'sent_length', 'true_word_index', 'span_id']\n",
    "        # df_pred = df_pred[req_cols]\n",
    "        # df_pred.loc[df_pred['true_word_index']==-1, 'true_word_index'] = 0\n",
    "        sent_pred = pd.DataFrame(df_pred.groupby(['sent_id', 'span_id'])['true_word_index'].min())\n",
    "        sent_pred['span_max_word_index'] = df_pred.groupby(['sent_id', 'span_id'])['true_word_index'].max()\n",
    "        sent_pred = sent_pred.rename(columns={'true_word_index':'span_min_word_index'}).reset_index()\n",
    "        #print(sent_pred)\n",
    "        submsn_df = pd.DataFrame()\n",
    "        for i, _id in enumerate(sent_pred['span_id'].tolist()):\n",
    "            #print(_id)\n",
    "            submsn_df.loc[i, 'article_id'] = article_ids[dev_id]\n",
    "            span_min_word = sent_pred.loc[sent_pred['span_id']==_id, 'span_min_word_index'].values[0]\n",
    "            #print(span_min_word)\n",
    "            span_max_word = sent_pred.loc[sent_pred['span_id']==_id, 'span_max_word_index'].values[0]\n",
    "            #print(span_max_word)\n",
    "            sentence_id = int(_id.split('_')[0])\n",
    "            sent_start_index = meta_dict[sentence_id]['start_index']\n",
    "            try:\n",
    "                submsn_df.loc[i, 'span_start'] = sent_start_index + meta_dict[sentence_id]['word_st_index'][span_min_word]\n",
    "                submsn_df.loc[i, 'span_end'] = sent_start_index + meta_dict[sentence_id]['word_en_index'][span_max_word] \n",
    "            except:\n",
    "                print(\"Escaping submsn_df :\" )\n",
    "                print(dev_id,_id)\n",
    "\n",
    "        return df_pred, sent_pred, submsn_df\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "dev_article_ids = [int(file.replace('article', '').replace('.txt', '')) for file in os.listdir(dev_articles_path)]\n",
    "thresh=0.8\n",
    "list_article=[]\n",
    "main_span_df = pd.DataFrame()\n",
    "for dev_id in tqdm(range(len(dev_article_ids))):\n",
    "    test_dataloader,test_sentences=prep_test_dataloader(dev_article_ids, dev_id, dev_articles_path)\n",
    "    padded_pred_tags_l,_=predict_bert(model,test_dataloader,test_sentences,thresh,threshold=False)\n",
    "    list_article.append(padded_pred_tags_l)\n",
    "    a, s,submsn_df = get_spans_article(dev_article_ids, dev_id, dev_articles_path,padded_pred_tags_l)\n",
    "    #print(dev_id)\n",
    "    #print(len(submsn_df))\n",
    "    main_span_df = pd.concat([main_span_df,submsn_df],sort=False,axis=0)\n",
    "print(main_span_df.shape)\n",
    "\n",
    "mdf=main_span_df[(main_span_df['span_end']>main_span_df['span_start'])]\n",
    "print(mdf.shape)\n",
    "\n",
    "_sub_ver='0'\n",
    "np.savetxt('bert_submsn_dev'+str(_sub_ver)+'.txt',mdf.values, fmt='%d', delimiter='\\t')\n",
    "\n",
    "with open('predictions_dev.pkl', 'wb') as f:\n",
    "    pickle.dump(list_article, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23 08:56:45,281 - INFO - Checking user submitted file\r\n",
      "2020-04-23 08:56:45,435 - INFO - Scoring the submission with precision and recall method\r\n",
      "2020-04-23 08:56:45,889 - INFO - Precision=430.793239/1330=0.323905\tRecall=558.129344/940=0.593755\r\n",
      "2020-04-23 08:56:45,889 - INFO - F1=0.419153\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/input/scoring/scoring/tools/task-SI_scorer.py -s /kaggle/working/bert_submsn_dev0.txt -r /kaggle/input/scoring/scoring/dev-labels-task1-span-identification/ -m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 87/90 [02:45<00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [02:51<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1669, 3)\n",
      "(1668, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test_article_ids = [int(file.replace('article', '').replace('.txt', '')) for file in os.listdir(test_articles_path)]\n",
    "thresh=0.8\n",
    "list_article=[]\n",
    "main_span_df = pd.DataFrame()\n",
    "for test_id in tqdm(range(len(test_article_ids))):\n",
    "    test_dataloader,test_sentences=prep_test_dataloader(test_article_ids, test_id, test_articles_path)\n",
    "    padded_pred_tags_l,_=predict_bert(model,test_dataloader,test_sentences,thresh,threshold=False)\n",
    "    list_article.append(padded_pred_tags_l)\n",
    "    a, s,submsn_df = get_spans_article(test_article_ids, test_id, test_articles_path,padded_pred_tags_l)\n",
    "    #print(dev_id)\n",
    "    #print(len(submsn_df))\n",
    "    main_span_df = pd.concat([main_span_df,submsn_df],sort=False,axis=0)\n",
    "print(main_span_df.shape)\n",
    "\n",
    "mdf=main_span_df[(main_span_df['span_end']>main_span_df['span_start'])]\n",
    "print(mdf.shape)\n",
    "\n",
    "_sub_ver='0'\n",
    "np.savetxt('bert_submsn_test'+str(_sub_ver)+'.txt',mdf.values, fmt='%d', delimiter='\\t')\n",
    "\n",
    "with open('predictions_test.pkl', 'wb') as f:\n",
    "    pickle.dump(list_article, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23 08:59:38,506 - INFO - Checking user submitted file\r\n",
      "2020-04-23 08:59:38,713 - INFO - Scoring the submission with precision and recall method\r\n",
      "2020-04-23 08:59:39,303 - INFO - Precision=699.328697/1668=0.419262\tRecall=728.083724/1379=0.527979\r\n",
      "2020-04-23 08:59:39,304 - INFO - F1=0.467382\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/input/scoring/scoring/tools/task-SI_scorer.py -s /kaggle/working/bert_submsn_test0.txt -r /kaggle/input/scoring/scoring/test-labels-task1-span-identification/ -m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01306ea4cc0448249e3761288fe09182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_26f1737c7a4841bb8610403f30673437",
       "placeholder": "​",
       "style": "IPY_MODEL_15b33ae99b9c46bf95c1030e269a0aa0",
       "value": " 361/361 [00:23&lt;00:00, 15.3B/s]"
      }
     },
     "0ee93a03d89d4e7aa5c2f61c7a2cc9f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f47ab97551184791a38080e1715dd223",
       "placeholder": "​",
       "style": "IPY_MODEL_14f8ff83e68a4afe946bf7a0b35cf903",
       "value": " 232k/232k [00:00&lt;00:00, 943kB/s]"
      }
     },
     "14f8ff83e68a4afe946bf7a0b35cf903": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "15b33ae99b9c46bf95c1030e269a0aa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "26f1737c7a4841bb8610403f30673437": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c1cf2c6be1345c2b5dbc5930540c134": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3d149cd5e0eb45b7868b7fa358ad7f1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ead1a9edf0d47d6bcf68cbd789d18b3",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3fb61ebe2e594ff99835037c4fbb28f1",
       "value": 231508
      }
     },
     "3fb61ebe2e594ff99835037c4fbb28f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "44e8ce8913a047d9a395177cb8a7adfb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5cb785c599564d258724d71dc245002a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_81d436b36d254b729d60b4a9f8f6cd64",
        "IPY_MODEL_cb13ef23e629491a8e7630f8c0480dbd"
       ],
       "layout": "IPY_MODEL_ba54e26272c04c8fa72cd91340822eb9"
      }
     },
     "6ead1a9edf0d47d6bcf68cbd789d18b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "794b4e4304ef4746a434d66ef1a170c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "810f5cb72e314960823b09ccfc7141c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "81d436b36d254b729d60b4a9f8f6cd64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d947a747ce42448083d17798a009c9db",
       "max": 440473133,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_94a1c8d73365466fa323d334cb851b71",
       "value": 440473133
      }
     },
     "94a1c8d73365466fa323d334cb851b71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "aa5737b3c72d46858cddc1c05b4b193f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b99a177712a847db888d9d3aa7b26999": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3d149cd5e0eb45b7868b7fa358ad7f1b",
        "IPY_MODEL_0ee93a03d89d4e7aa5c2f61c7a2cc9f5"
       ],
       "layout": "IPY_MODEL_aa5737b3c72d46858cddc1c05b4b193f"
      }
     },
     "ba54e26272c04c8fa72cd91340822eb9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb13ef23e629491a8e7630f8c0480dbd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44e8ce8913a047d9a395177cb8a7adfb",
       "placeholder": "​",
       "style": "IPY_MODEL_3c1cf2c6be1345c2b5dbc5930540c134",
       "value": " 440M/440M [00:12&lt;00:00, 36.6MB/s]"
      }
     },
     "d947a747ce42448083d17798a009c9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f00c8ca416ba41c78612e3e3f8635746": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f4895a0bc46c4a7f80fcbc5e16c0c294",
        "IPY_MODEL_01306ea4cc0448249e3761288fe09182"
       ],
       "layout": "IPY_MODEL_794b4e4304ef4746a434d66ef1a170c2"
      }
     },
     "f300ade50f114439a03fdce0cf7553a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f47ab97551184791a38080e1715dd223": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4895a0bc46c4a7f80fcbc5e16c0c294": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f300ade50f114439a03fdce0cf7553a3",
       "max": 361,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_810f5cb72e314960823b09ccfc7141c5",
       "value": 361
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
