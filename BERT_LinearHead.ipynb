{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel,BertModel, BertConfig\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "class BertForTokenClassificationWeighted(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "#         sequence_output = self.dropout(sequence_output)\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.classifier(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.classifier(dropout(sequence_output))\n",
    "        logits=logits/ len(self.dropouts)\n",
    "#         logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            w = torch.tensor([1.,10.,1. ]).cuda()\n",
    "            loss_fct = CrossEntropyLoss(weight=w)\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south\n",
      "florida\n",
      "muslim\n",
      "leader\n",
      "sofian\n",
      "zakkout\n",
      "david\n",
      "duke\n",
      "day\n",
      "south florida muslim leader sofian zakkout david duke day\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729194c39fa54c20b22d5d5c5f0ebe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "south florida muslim leader sofian zakkout david duke day\n",
      "['south', 'florida', 'muslim', 'leader', 'sofia', '##n', 'za', '##kko', '##ut', 'david', 'duke', 'day']\n",
      "(14885, 128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eab8072ccb461986fbbfaea03d2cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555c4e58a2c8402e99ba5be09783f32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6037662888161615\n",
      "Validation loss: 0.5189673063846735\n",
      "Validation Accuracy: 0.6697955364947552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  14%|█▍        | 1/7 [03:14<19:26, 194.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4283294299982736\n",
      "Train loss: 0.42096592700948277\n",
      "Validation loss: 0.6780691146850586\n",
      "Validation Accuracy: 0.7598377574573865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  29%|██▊       | 2/7 [06:28<16:11, 194.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4613391836960526\n",
      "Train loss: 0.2814882308596449\n",
      "Validation loss: 0.9156226713496906\n",
      "Validation Accuracy: 0.7663881528627622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  43%|████▎     | 3/7 [09:42<12:57, 194.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4526447275277791\n",
      "Train loss: 0.19998299794135688\n",
      "Validation loss: 1.6412972945433397\n",
      "Validation Accuracy: 0.8524536986451049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|█████▋    | 4/7 [12:57<09:42, 194.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.47795949573071295\n",
      "Train loss: 0.1557157724404252\n",
      "Validation loss: 1.5761998381752234\n",
      "Validation Accuracy: 0.8523670543323865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  71%|███████▏  | 5/7 [16:11<06:28, 194.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4949731402888808\n",
      "Train loss: 0.12432599624083278\n",
      "Validation loss: 1.553738803244554\n",
      "Validation Accuracy: 0.827399834052666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  86%|████████▌ | 6/7 [19:24<03:14, 194.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.49024139553171703\n",
      "Train loss: 0.10197410229273748\n",
      "Validation loss: 1.6502972107667189\n",
      "Validation Accuracy: 0.8445131528627622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 7/7 [22:38<00:00, 194.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.4989902663326185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig,WordpieceTokenizer\n",
    "from transformers import DistilBertForTokenClassification,DistilBertTokenizer,DistilBertConfig\n",
    "from transformers import RobertaForTokenClassification,RobertaTokenizer,RobertaConfig\n",
    "from transformers import BertForTokenClassification,BertTokenizer,BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "global max_len\n",
    "data = pd.read_csv(\"../input/datapropcsv/data_prop.csv\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "data['label'] = data['label'].astype(str)\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p) for w, p in zip(s[\"word_corrected\"].values.tolist(),\n",
    "                                                           s[\"label\"].values.tolist())]\n",
    "        \n",
    "        self.grouped = self.data.groupby(\"sent_id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "\n",
    "\n",
    "sent = getter.sentences\n",
    "\n",
    "for s in sent[0]:\n",
    "    print(s[0].split()[0])\n",
    "    \n",
    "\n",
    "##keeping only the first word after removing the special characters\n",
    "sentences = [\" \".join([s[0].split()[0] for s in sent]) for sent in getter.sentences]\n",
    "print(sentences[0])\n",
    "\n",
    "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])\n",
    "\n",
    "# tags_vals = list(set(data[\"label\"].values))\n",
    "# tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,WeightedRandomSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_LEN = 128\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print(n_gpu)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "### Now we tokenize all sentences\n",
    "\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "print(sentences[0])\n",
    "print(tokenized_texts[0])\n",
    "\n",
    "def reg_encoding(cleaned: list, labels: list, hash_token:list) -> list:\n",
    "    label_l = []\n",
    "    for oindex, x in enumerate(cleaned):\n",
    "        #print(oindex)\n",
    "        tlist = []        \n",
    "        i=0\n",
    "        j=0\n",
    "        while i < len(x): \n",
    "            if x[i][0]=='#':        \n",
    "                tlist.append(hash_token)\n",
    "            else:\n",
    "                #print(x[i])\n",
    "                tlist.append(labels[oindex][j])\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "            \n",
    "        label_l.append(tlist)\n",
    "    return label_l\n",
    "\n",
    "def reg_encoding_generic(sentences,labels, hash_token):\n",
    "    tokens_all=[]\n",
    "    labels_all=[]\n",
    "    for (sentence,label) in zip(sentences,labels):\n",
    "        tokens_per_sentence=[]\n",
    "        labels_per_sentence=[]\n",
    "        for word, label in zip(sentence.split(), label):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens_per_sentence.extend(word_tokens)\n",
    "                labels_per_sentence.extend([label] + [hash_token] * (len(word_tokens) - 1))\n",
    "#                 labels_per_sentence.extend([label] + [label] * (len(word_tokens) - 1))\n",
    "\n",
    "        \n",
    "        tokens_all.append(tokens_per_sentence)\n",
    "        labels_all.append(labels_per_sentence)\n",
    "                \n",
    "    return tokens_all,labels_all\n",
    "\n",
    "def reg_encoding_modify(cleaned: list, labels: list, hash_token, end_token) -> list:\n",
    "    label_l = []\n",
    "    for oindex, x in enumerate(cleaned):\n",
    "        #print(oindex)\n",
    "        tlist = []        \n",
    "        i=0\n",
    "        j=0\n",
    "        while i < len(x): \n",
    "            if x[i][0]=='#':        \n",
    "                #tlist.append(hash_token)\n",
    "                tlist.append(labels[oindex][j-1])\n",
    "                \n",
    "            else:\n",
    "                #print(x[i])\n",
    "                tlist.append(labels[oindex][j])\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "            \n",
    "        label_l.append(tlist)\n",
    "    return label_l\n",
    "\n",
    "\n",
    "tokenized_texts,label_l = reg_encoding_generic(sentences,labels,'X')\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in label_l for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#label_l_modify = reg_encoding_modify(tokenized_texts,labels,'X','E')\n",
    "\n",
    "# data['label'].unique()\n",
    "\n",
    "tags_vals=['0','1','X']\n",
    "tag2idx={'0':0,'1':1,'X':2}\n",
    "\n",
    "# tags_vals=['0','1']\n",
    "# tag2idx={'0':0,'1':1}\n",
    "\n",
    "\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in label_l],maxlen=MAX_LEN, value =0, padding=\"post\",dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2018, test_size=0.1)\n",
    "\n",
    "sent_labels=np.zeros(tr_tags.shape[0],dtype=int)\n",
    "for i in range(len(tr_tags)):\n",
    "    if 1 in tr_tags[i,:]:\n",
    "        sent_labels[i]=1\n",
    "    \n",
    "del input_ids,tags,attention_masks\n",
    "print(tr_inputs.shape)\n",
    "\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags) \n",
    "train_sampler = RandomSampler(train_data)\n",
    "_,class_sample_count=np.unique( sent_labels , return_counts=True)   \n",
    "class_sample_counts=list(class_sample_count)\n",
    "# class_sample_count = [10, 1, 20, 3, 4] # dataset has 10 class-1 samples, 1 class-2 samples, etc.\n",
    "# weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "# samples_weights = weights[sent_labels]\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, num_samples=len(samples_weights),replacement=True)\n",
    "# train_dataloader = DataLoader(train_data, batch_size = bs, sampler = sampler)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "\n",
    "\n",
    "# model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(tag2idx))\n",
    "# model = RobertaForTokenClassification.from_pretrained(\"roberta-base\", num_labels=len(tag2idx))\n",
    "\n",
    "# model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "model = BertForTokenClassificationWeighted.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2, num_training_steps=5)\n",
    "# opt = SWA(optimizer, swa_start=10, swa_freq=2, swa_lr=0.05)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "epochs = 7\n",
    "max_grad_norm = 1.0\n",
    "tmp=0\n",
    "\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "#         loss = model(b_input_ids, token_type_ids=None,\n",
    "#                      attention_mask=b_input_mask, labels=b_labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         opt.step()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "#     opt.swap_swa_sgd()\n",
    "\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    pred=[]\n",
    "    \n",
    "    for batch in (valid_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "#             tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "#                                   attention_mask=b_input_mask, labels=b_labels)\n",
    "#             logits = model(b_input_ids, token_type_ids=None,\n",
    "#                            attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        #print(logits.shape)\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        #\n",
    "        pred.append(list(p) for p in np.argmax(logits, axis=2))\n",
    "        \n",
    "        #\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    \n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    \n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    \n",
    "    torch.save(model.state_dict(), 'bert_uncased_attentionmask.pth')\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    tmp_f1=f1_score(pred_tags , valid_tags,average='macro')\n",
    "    if tmp_f1>tmp:\n",
    "        torch.save(model.state_dict(), 'bert_uncased_SI.pth')\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags , valid_tags,average='macro')))\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 49/75 [02:13<01:09,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 59/75 [02:37<00:35,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [03:14<00:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160, 3)\n",
      "(1160, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading the model\n",
    "# model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))    \n",
    "\n",
    "# model.load_state_dict(torch.load('/kaggle/output/bert-new/bert_uncased_SI.pth'))\n",
    "# model.load_state_dict(torch.load('/kaggle/working/bert-new/bert_uncased_SI.pth'))\n",
    "\n",
    "\n",
    "# model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# Fetching test data\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text= text.lower()\n",
    "    text= re.sub(r'[^a-z]',' ',text)\n",
    "    text= ' '.join(text.split())\n",
    "    return(text)\n",
    "def get_wordchar_indicies(sent):\n",
    "    k_l = list(sent)\n",
    "    k_l_b = [0 if i==' ' else 1 for i in k_l]\n",
    "    k_df = pd.DataFrame({'char':k_l, 'space_mark':k_l_b})\n",
    "    k_df = k_df.reset_index()\n",
    "    k_df['u1'] = k_df['space_mark'].diff()\n",
    "    k_df['u1'].fillna(1, inplace=True)\n",
    "    k_df.loc[k_df['u1']==1, 'u2']= k_df.loc[k_df['u1']==1, 'u1'].cumsum()\n",
    "    k_df.loc[k_df['u1']==-1, 'u2']= k_df.loc[k_df['u1']==-1, 'u1']\n",
    "    k_df['u2'] = k_df['u2'].ffill(axis=0)\n",
    "    k_df = k_df[k_df['u2']!=-1]\n",
    "    k_df_gb = pd.DataFrame(k_df.groupby(['u2'])['index'].min())\n",
    "    k_df_gb['last_index_word'] = k_df.groupby(['u2'])['index'].max()\n",
    "    k_df_gb = k_df_gb.reset_index().rename(columns={'u2':'word_index','index':'first_index_word'})\n",
    "    try:\n",
    "        k_df_gb['words'] = sent.split()\n",
    "    except:\n",
    "        print(k_df_gb, sent)\n",
    "    return k_df_gb\n",
    "\n",
    "def indices_sentence(article_id,path):\n",
    "    f= open(path + 'article' + str(article_id) + '.txt',\"r\")\n",
    "    indices={}\n",
    "    start_index = 0\n",
    "    for i, line in enumerate(f):\n",
    "        indices[i] = {}\n",
    "        indices[i]['article_id']=article_id\n",
    "        indices[i]['span_present'] = 0\n",
    "        indices[i]['sentence'] = line\n",
    "        indices[i]['start_index'] = start_index\n",
    "        indices[i]['end_index'] = start_index + len(line)\n",
    "        start_index = indices[i]['end_index']   \n",
    "        \n",
    "        if line == '\\n':\n",
    "            indices[i]['word_st_index'] = [0]\n",
    "            indices[i]['word_en_index'] = [0]\n",
    "        else:\n",
    "            wordchar_df = get_wordchar_indicies(line)\n",
    "            indices[i]['word_st_index'] = list(wordchar_df['first_index_word'])\n",
    "            indices[i]['word_en_index'] = list(wordchar_df['last_index_word'])        \n",
    "        \n",
    "    return indices\n",
    "def get_test_sentences(article_ids, dev_id, articles_path):\n",
    "    se_dict=indices_sentence(article_ids[dev_id],articles_path)\n",
    "    test_sent=[]\n",
    "    for i in range(len(se_dict)):\n",
    "                   test_sent.append(se_dict[i]['sentence'])\n",
    "    return se_dict, test_sent\n",
    "\n",
    "\n",
    "\n",
    "def prep_test_dataloader(article_ids, dev_id, articles_path):\n",
    "    se_dict,test_sentences=get_test_sentences(article_ids,dev_id,articles_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    tokenized_test_sentences = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
    "    input_test_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_test_sentences],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_test_masks = [[float(i>0) for i in ii] for ii in input_test_ids]\n",
    "    test_inputs = torch.tensor(input_test_ids)\n",
    "\n",
    "    test_masks = torch.tensor(attention_test_masks)\n",
    "    test_data = TensorDataset(test_inputs, test_masks)\n",
    "\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
    "    return test_dataloader,test_sentences\n",
    "\n",
    "\n",
    "def inverse_reg_enco(sentences,labels_tok):\n",
    "    label_inverse=[]\n",
    "    for i,sent in enumerate(sentences) :\n",
    "        label_o=[]\n",
    "        word_count=0\n",
    "        tok_count=0\n",
    "        for word in sent.split():            \n",
    "            #print(word)\n",
    "            tok=tokenizer.tokenize(word)\n",
    "            tok_count=len(tok)+tok_count\n",
    "            #print(tok_count)\n",
    "            if tok_count>MAX_LEN:\n",
    "                break    \n",
    "            #print(word_count)\n",
    "            if len(tok)>1:\n",
    "                #print('True')\n",
    "                label_word_token=labels_tok[i][word_count:word_count+len(tok)]\n",
    "                #print(label_word_token)\n",
    "                if '1' in label_word_token:\n",
    "                    label_o.append('1')\n",
    "                else:\n",
    "                    label_o.append('0')\n",
    "                word_count=word_count+len(tok)\n",
    "            else:\n",
    "                #print(labels_tok[i][word_count])\n",
    "                label_o.append(labels_tok[i][word_count])\n",
    "                word_count=word_count+1    \n",
    "        label_inverse.append(label_o)\n",
    "    return label_inverse\n",
    "\n",
    "# out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "# preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "# def inverse_reg_enco_modify(sentences,labels_tok):\n",
    "#     for i in range(labels_tok.shape[0]):\n",
    "#         for j in range(labels_tok.shape[1]):\n",
    "#             if labels_tok[i, j] != 2:\n",
    "#                 out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "#                 preds_list[i].append(label_map[preds[i][j]])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def thresh_logit(logits,thresh):\n",
    "    predictions_per_batch=[]\n",
    "    for i in range(logits.shape[0]):\n",
    "        predictions_per_sentence=[]\n",
    "        for j in range(logits.shape[1]):\n",
    "            if (np.argmax(logits[i][j])==1 and logits[i][j][1]>=thresh):\n",
    "                predictions_per_sentence.append(1)\n",
    "            else:\n",
    "                predictions_per_sentence.append(0)\n",
    "        predictions_per_batch.append(predictions_per_sentence)\n",
    "    return predictions_per_batch \n",
    "\n",
    "\n",
    "#from seqeval.metrics import f1_score\n",
    "def predict_bert(model,test_dataloader,test_sentences,thresh,threshold=False):\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    l=[]\n",
    "    #true_labels = []\n",
    "    #eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
    "        with torch.no_grad():\n",
    "            #tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "            #                      attention_mask=b_input_mask, labels=b_labels)\n",
    "#             logits = model(b_input_ids, token_type_ids=None,\n",
    "#                            attention_mask=b_input_mask)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "            \n",
    "\n",
    "        m=torch.nn.Softmax(dim=2)\n",
    "        logits_softmax=m(logits)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        logits_softmax = logits_softmax.detach().cpu().numpy()\n",
    "        #print(logits.shape)\n",
    "        #print(logits_softmax.shape)\n",
    "        #l.extend(logits_softmax)\n",
    "        if threshold:\n",
    "            predictions_per_batch=thresh_logit(logits,thresh)\n",
    "            predictions.extend(predictions_per_batch)\n",
    "        else:\n",
    "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "            \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "    pred_tags_l=inverse_reg_enco(test_sentences,pred_tags)\n",
    "    padded_pred_tags_l=pad_sequences([[tag2idx.get(l) for l in lab] for lab in pred_tags_l],\n",
    "                     maxlen=MAX_LEN, value = tag2idx['0'], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "    return padded_pred_tags_l,pred_tags\n",
    "\n",
    "\n",
    "def get_spans_article(article_ids, dev_id, articles_path,padded_pred_tags_l):\n",
    "    pad_style='Post'\n",
    "    spangaps_to_merge = [1, 2]\n",
    "    def get_truewordindex(wi, sl):\n",
    "        if sl>=max_len:\n",
    "            return wi\n",
    "        else:\n",
    "            return wi-(max_len-sl)\n",
    "    ar=np.array(padded_pred_tags_l)\n",
    "    #print(padded_pred_tags_l.shape)\n",
    "    a,b=np.where(ar==1)\n",
    "    \n",
    "    df_pred=pd.DataFrame({'sent_id':a,'word_index':b})\n",
    "    #print(len(df_pred))\n",
    "    meta_dict, test_sentences= get_test_sentences(article_ids, dev_id, articles_path)\n",
    "    org_sent_len = len(test_sentences)\n",
    "\n",
    "    len_sentence={i:len(test_sentences[i].split()) for i in range(len(test_sentences))}\n",
    "    #print(len_sentence)\n",
    "    df_pred['sent_length']=df_pred['sent_id'].map(len_sentence)\n",
    "    df_pred = df_pred[df_pred['sent_id']<org_sent_len]\n",
    "\n",
    "    # if len(df_pred)==0:\n",
    "    #     return df_pred, df_pred, df_pred\n",
    "    # df_pred['true_word_index']=df_pred['word_index']-(max_len-df_pred['sent_length'])\n",
    "    if pad_style == 'pre':\n",
    "        df_pred['true_word_index'] = list(map(lambda x, y: get_truewordindex(x, y) , \n",
    "                                          df_pred['word_index'], df_pred['sent_length']))\n",
    "    else:\n",
    "        df_pred['true_word_index'] = df_pred['word_index'].values\n",
    "    #print(df_pred)\n",
    "    df_pred = df_pred[~(df_pred['true_word_index']>=df_pred['sent_length'])] \n",
    "    \n",
    "    #print(len(df_pred))\n",
    "    df_pred['diff_pred']=df_pred.groupby(['sent_id'])['true_word_index'].diff()\n",
    "    df_pred['diff_pred'] = df_pred['diff_pred'].apply(\n",
    "                        lambda x:-1 if x in spangaps_to_merge else np.nan)\n",
    "    #print (len(df_pred))\n",
    "    #print(df_pred)\n",
    "    if len(df_pred)<1:\n",
    "        print(\"Escaping empty dataframe :\")\n",
    "        print(dev_id)\n",
    "        submsn_df=pd.DataFrame()\n",
    "        sent_pred=pd.DataFrame()\n",
    "        return df_pred, sent_pred, submsn_df\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        df_pred.loc[df_pred['diff_pred'].isnull(), 'diff_pred_1'] = \\\n",
    "                    df_pred.groupby(['sent_id'])['diff_pred'].cumcount()\n",
    "        df_pred['diff_pred_1'] = df_pred['diff_pred_1'].ffill(axis=0)\n",
    "        df_pred['span_id'] = list(map(lambda x, y: str(int(x)) + '_' + str(int(y)), \n",
    "                                      df_pred['sent_id'], df_pred['diff_pred_1']))\n",
    "        # req_cols = ['sent_id', 'word_index', 'sent_length', 'true_word_index', 'span_id']\n",
    "        # df_pred = df_pred[req_cols]\n",
    "        # df_pred.loc[df_pred['true_word_index']==-1, 'true_word_index'] = 0\n",
    "        sent_pred = pd.DataFrame(df_pred.groupby(['sent_id', 'span_id'])['true_word_index'].min())\n",
    "        sent_pred['span_max_word_index'] = df_pred.groupby(['sent_id', 'span_id'])['true_word_index'].max()\n",
    "        sent_pred = sent_pred.rename(columns={'true_word_index':'span_min_word_index'}).reset_index()\n",
    "        #print(sent_pred)\n",
    "        submsn_df = pd.DataFrame()\n",
    "        for i, _id in enumerate(sent_pred['span_id'].tolist()):\n",
    "            #print(_id)\n",
    "            submsn_df.loc[i, 'article_id'] = article_ids[dev_id]\n",
    "            span_min_word = sent_pred.loc[sent_pred['span_id']==_id, 'span_min_word_index'].values[0]\n",
    "            #print(span_min_word)\n",
    "            span_max_word = sent_pred.loc[sent_pred['span_id']==_id, 'span_max_word_index'].values[0]\n",
    "            #print(span_max_word)\n",
    "            sentence_id = int(_id.split('_')[0])\n",
    "            sent_start_index = meta_dict[sentence_id]['start_index']\n",
    "            try:\n",
    "                submsn_df.loc[i, 'span_start'] = sent_start_index + meta_dict[sentence_id]['word_st_index'][span_min_word]\n",
    "                submsn_df.loc[i, 'span_end'] = sent_start_index + meta_dict[sentence_id]['word_en_index'][span_max_word] \n",
    "            except:\n",
    "                print(\"Escaping submsn_df :\" )\n",
    "                print(dev_id,_id)\n",
    "\n",
    "        return df_pred, sent_pred, submsn_df\n",
    "\n",
    "\n",
    "import os\n",
    "dev_articles_path = '/kaggle/input/dev-articles/dev-articles/'\n",
    "dev_article_ids = [int(file.replace('article', '').replace('.txt', '')) for file in os.listdir(dev_articles_path)]\n",
    "thresh=0.8\n",
    "list_article=[]\n",
    "main_span_df = pd.DataFrame()\n",
    "for dev_id in tqdm(range(len(dev_article_ids))):\n",
    "    test_dataloader,test_sentences=prep_test_dataloader(dev_article_ids, dev_id, dev_articles_path)\n",
    "    padded_pred_tags_l,_=predict_bert(model,test_dataloader,test_sentences,thresh,threshold=False)\n",
    "    list_article.append(padded_pred_tags_l)\n",
    "    a, s,submsn_df = get_spans_article(dev_article_ids, dev_id, dev_articles_path,padded_pred_tags_l)\n",
    "    #print(dev_id)\n",
    "    #print(len(submsn_df))\n",
    "    main_span_df = pd.concat([main_span_df,submsn_df],sort=False,axis=0)\n",
    "print(main_span_df.shape)\n",
    "\n",
    "mdf=main_span_df[(main_span_df['span_end']>main_span_df['span_start'])]\n",
    "print(mdf.shape)\n",
    "\n",
    "_sub_ver='0'\n",
    "np.savetxt('bert_submsn_dev'+str(_sub_ver)+'.txt',mdf.values, fmt='%d', delimiter='\\t')\n",
    "\n",
    "with open('predictions_dev.pkl', 'wb') as f:\n",
    "    pickle.dump(list_article, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-22 15:52:39,893 - INFO - Checking user submitted file\r\n",
      "2020-04-22 15:52:40,047 - INFO - Scoring the submission with precision and recall method\r\n",
      "2020-04-22 15:52:40,460 - INFO - Precision=406.268768/1160=0.350232\tRecall=500.005057/940=0.531920\r\n",
      "2020-04-22 15:52:40,460 - INFO - F1=0.422366\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/input/scoring/scoring/tools/task-SI_scorer.py -s /kaggle/working/bert_submsn_dev0.txt -r /kaggle/input/scoring/scoring/dev-labels-task1-span-identification/ -m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/90 [00:47<02:24,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 37/90 [01:31<02:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [02:45<00:32,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 76/90 [02:59<00:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escaping empty dataframe :\n",
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [03:30<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1544, 3)\n",
      "(1542, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test_articles_path = '/kaggle/input/testarticles/test-articles/'\n",
    "test_article_ids = [int(file.replace('article', '').replace('.txt', '')) for file in os.listdir(test_articles_path)]\n",
    "thresh=0.8\n",
    "list_article=[]\n",
    "main_span_df = pd.DataFrame()\n",
    "for test_id in tqdm(range(len(test_article_ids))):\n",
    "    test_dataloader,test_sentences=prep_test_dataloader(test_article_ids, test_id, test_articles_path)\n",
    "    padded_pred_tags_l,_=predict_bert(model,test_dataloader,test_sentences,thresh,threshold=False)\n",
    "    list_article.append(padded_pred_tags_l)\n",
    "    a, s,submsn_df = get_spans_article(test_article_ids, test_id, test_articles_path,padded_pred_tags_l)\n",
    "    #print(dev_id)\n",
    "    #print(len(submsn_df))\n",
    "    main_span_df = pd.concat([main_span_df,submsn_df],sort=False,axis=0)\n",
    "print(main_span_df.shape)\n",
    "\n",
    "mdf=main_span_df[(main_span_df['span_end']>main_span_df['span_start'])]\n",
    "print(mdf.shape)\n",
    "\n",
    "_sub_ver='0'\n",
    "np.savetxt('bert_submsn_test'+str(_sub_ver)+'.txt',mdf.values, fmt='%d', delimiter='\\t')\n",
    "\n",
    "with open('predictions_test.pkl', 'wb') as f:\n",
    "    pickle.dump(list_article, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-22 15:56:11,701 - INFO - Checking user submitted file\r\n",
      "2020-04-22 15:56:11,940 - INFO - Scoring the submission with precision and recall method\r\n",
      "2020-04-22 15:56:12,536 - INFO - Precision=690.555317/1542=0.447831\tRecall=674.264965/1379=0.488952\r\n",
      "2020-04-22 15:56:12,536 - INFO - F1=0.467489\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /kaggle/input/scoring/scoring/tools/task-SI_scorer.py -s /kaggle/working/bert_submsn_test0.txt -r /kaggle/input/scoring/scoring/test-labels-task1-span-identification/ -m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03ce164b52604bfd91d507e2c819adab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f8c7c1394314b47a43ef53ce4847eae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_baa1b351909641b9beda14b7313231e9",
       "max": 440473133.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ff2edfb23d73437091e840abed334273",
       "value": 440473133.0
      }
     },
     "0fc42dc6b83b4aba966c96076fcedd76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1e6c74a871a94d16807dcc96321b4d3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2af03be5b35e48e5859a9682f2fd5603": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30ec084fa9574f62ae7f87da98cdf1d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2af03be5b35e48e5859a9682f2fd5603",
       "placeholder": "​",
       "style": "IPY_MODEL_f6c87cec0ff04f75929c02af347ecd09",
       "value": " 361/361 [00:51&lt;00:00, 7.00B/s]"
      }
     },
     "42949669669a4c6a9ed0e49d481949e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "45e7d1df991848e49f863fb4c737a041": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e2a16109881a4dfbb85e4064ca2727e3",
       "max": 231508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0fc42dc6b83b4aba966c96076fcedd76",
       "value": 231508.0
      }
     },
     "4c68d2b27bba43dab8b4f83a26f93796": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "555c4e58a2c8402e99ba5be09783f32a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0f8c7c1394314b47a43ef53ce4847eae",
        "IPY_MODEL_e7f703ce5af94a5daa85ee03c77d940b"
       ],
       "layout": "IPY_MODEL_a5faaad7c7cb44a4a01760071ec03a7e"
      }
     },
     "729194c39fa54c20b22d5d5c5f0ebe8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_45e7d1df991848e49f863fb4c737a041",
        "IPY_MODEL_d750f578701a4a198555560dbd941bda"
       ],
       "layout": "IPY_MODEL_c4e96d038fc74d7d9570fa1b5d9a2d9a"
      }
     },
     "a1b506c7bbf04a1ab0a2fe18eed4ae78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a5faaad7c7cb44a4a01760071ec03a7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8458879494949ae8c086ff5e49f7673": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8e22f0cf3ec46ed976dfb40bb76f278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_03ce164b52604bfd91d507e2c819adab",
       "max": 361.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c4838797a50148bea38d50435b82ddab",
       "value": 361.0
      }
     },
     "baa1b351909641b9beda14b7313231e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4838797a50148bea38d50435b82ddab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c4e96d038fc74d7d9570fa1b5d9a2d9a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d750f578701a4a198555560dbd941bda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4c68d2b27bba43dab8b4f83a26f93796",
       "placeholder": "​",
       "style": "IPY_MODEL_b8458879494949ae8c086ff5e49f7673",
       "value": " 232k/232k [00:42&lt;00:00, 5.44kB/s]"
      }
     },
     "e2a16109881a4dfbb85e4064ca2727e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7f703ce5af94a5daa85ee03c77d940b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_42949669669a4c6a9ed0e49d481949e6",
       "placeholder": "​",
       "style": "IPY_MODEL_a1b506c7bbf04a1ab0a2fe18eed4ae78",
       "value": " 440M/440M [00:39&lt;00:00, 11.2MB/s]"
      }
     },
     "e8eab8072ccb461986fbbfaea03d2cc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b8e22f0cf3ec46ed976dfb40bb76f278",
        "IPY_MODEL_30ec084fa9574f62ae7f87da98cdf1d0"
       ],
       "layout": "IPY_MODEL_1e6c74a871a94d16807dcc96321b4d3a"
      }
     },
     "f6c87cec0ff04f75929c02af347ecd09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ff2edfb23d73437091e840abed334273": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
